{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b76704eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L 4-23-25\n",
    "\n",
    "# notebooks/2_MFCC_RF_Classifier.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13b82b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Errors are expected:\n",
      "Some of the encodings can vary\n",
      "Some of the audio files are corrupted\n",
      "\n",
      "Loading metadata...\n",
      "Extracting MFCC features...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "import joblib\n",
    "from collections import Counter\n",
    "\n",
    "# Paths\n",
    "METADATA_PATH = \"../reports/1_Explore_Metadata/valid_track_genres.csv\"\n",
    "AUDIO_DIR = \"../data/fma_small/\"\n",
    "FEATURE_SAVE_PATH = \"../features/mfcc_features.npz\"\n",
    "MODEL_SAVE_PATH = \"../models/rf_model.pkl\"\n",
    "TEST_OUTPUTS_PATH = \"../features/rf_test_outputs.npz\"\n",
    "REPORT_DIR = \"../reports/2_MFCC_RF_Classifier/\"\n",
    "os.makedirs(REPORT_DIR, exist_ok=True)\n",
    "\n",
    "# Output paths\n",
    "SUCCESS_CSV = os.path.join(REPORT_DIR, \"used_tracks.csv\")\n",
    "FAILED_CSV = os.path.join(REPORT_DIR, \"failed_tracks.csv\")\n",
    "GENRE_COUNTS_CSV = os.path.join(REPORT_DIR, \"genre_subgenre_counts.csv\")\n",
    "FULL_CLASS_REPORT_TXT = os.path.join(\n",
    "    REPORT_DIR, \"rf_classification_report.txt\")\n",
    "\n",
    "# Load track genre metadata\n",
    "\n",
    "\n",
    "def load_metadata(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.dropna(subset=['genre_top'])\n",
    "    return df\n",
    "\n",
    "# Extract MFCCs from a single audio file\n",
    "\n",
    "\n",
    "def extract_mfcc(path, n_mfcc=20):\n",
    "    try:\n",
    "        y, sr = librosa.load(path, sr=None, duration=30)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "        return np.mean(mfcc, axis=1), np.std(mfcc, axis=1)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Main pipeline\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Note: Errors are expected:\")\n",
    "    print(\"Some of the encodings can vary\")\n",
    "    print(\"Some of the audio files are corrupted\\n\")\n",
    "\n",
    "    print(\"Loading metadata...\")\n",
    "    df = load_metadata(METADATA_PATH)\n",
    "    track_ids = df['track_id'].astype(str).str.zfill(6).tolist()\n",
    "    labels = df['genre_top'].tolist()\n",
    "    subgenres_list = df['subgenres'].apply(eval).tolist()\n",
    "\n",
    "    print(\"Extracting MFCC features...\")\n",
    "    X, y, used_info = [], [], []\n",
    "    failed_info = []\n",
    "\n",
    "    for idx, track_id in enumerate(track_ids):\n",
    "        subfolder = track_id[:3]\n",
    "        filename = f\"{track_id}.mp3\"\n",
    "        filepath = os.path.normpath(\n",
    "            os.path.join(AUDIO_DIR, subfolder, filename))\n",
    "\n",
    "        genre = labels[idx]\n",
    "        subgenres = subgenres_list[idx]\n",
    "\n",
    "        if not os.path.isfile(filepath):\n",
    "            failed_info.append({\n",
    "                'track_id': track_id,\n",
    "                'reason': 'file not found',\n",
    "                'genre': genre,\n",
    "                'subgenres': subgenres\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        mfcc_mean, mfcc_std = extract_mfcc(filepath)\n",
    "        if mfcc_mean is not None:\n",
    "            feature_vector = np.concatenate([mfcc_mean, mfcc_std])\n",
    "            X.append(feature_vector)\n",
    "            y.append(genre)\n",
    "            used_info.append({\n",
    "                'track_id': track_id,\n",
    "                'genre': genre,\n",
    "                'subgenres': subgenres\n",
    "            })\n",
    "        else:\n",
    "            failed_info.append({\n",
    "                'track_id': track_id,\n",
    "                'reason': 'processing error',\n",
    "                'genre': genre,\n",
    "                'subgenres': subgenres\n",
    "            })\n",
    "\n",
    "    # Convert to arrays and save\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    np.savez(FEATURE_SAVE_PATH, X=X, y=y)\n",
    "    print(f\"Saved {len(X)} feature vectors and labels to {FEATURE_SAVE_PATH}\")\n",
    "\n",
    "    # Save usage and error logs\n",
    "    pd.DataFrame(used_info).to_csv(SUCCESS_CSV, index=False)\n",
    "    pd.DataFrame(failed_info).to_csv(FAILED_CSV, index=False)\n",
    "    print(f\"Saved used track info to {SUCCESS_CSV}\")\n",
    "    print(f\"Saved failed track info to {FAILED_CSV}\")\n",
    "\n",
    "    # Count genre and subgenre stats\n",
    "    genre_counts = Counter([row['genre'] for row in used_info])\n",
    "    all_subgenres = [sub for row in used_info for sub in row['subgenres']]\n",
    "    subgenre_counts = Counter(all_subgenres)\n",
    "\n",
    "    genre_df = pd.DataFrame(list(genre_counts.items()),\n",
    "                            columns=[\"label\", \"count\"])\n",
    "    genre_df[\"type\"] = \"genre\"\n",
    "\n",
    "    subgenre_df = pd.DataFrame(\n",
    "        list(subgenre_counts.items()), columns=[\"label\", \"count\"])\n",
    "    subgenre_df[\"type\"] = \"subgenre\"\n",
    "\n",
    "    pd.concat([genre_df, subgenre_df], ignore_index=True).to_csv(\n",
    "        GENRE_COUNTS_CSV, index=False)\n",
    "    print(f\"Saved genre/subgenre counts to {GENRE_COUNTS_CSV}\")\n",
    "\n",
    "    # Train/Test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "    print(\"Training Random Forest classifier...\")\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    joblib.dump(clf, MODEL_SAVE_PATH)\n",
    "    print(f\"Saved model to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "    # Predict and evaluate\n",
    "    y_pred = clf.predict(X_test)\n",
    "    np.savez(TEST_OUTPUTS_PATH, X_test=X_test, y_test=y_test, y_pred=y_pred)\n",
    "    print(f\"Saved test predictions to {TEST_OUTPUTS_PATH}\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    labels_sorted = sorted(set(y))\n",
    "    report = classification_report(\n",
    "        y_test, y_pred, labels=labels_sorted, zero_division=0)\n",
    "    print(report)\n",
    "\n",
    "    with open(FULL_CLASS_REPORT_TXT, \"w\") as f:\n",
    "        f.write(report)\n",
    "    print(f\"Saved classification report to {FULL_CLASS_REPORT_TXT}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=labels_sorted)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, cmap='Blues', interpolation='nearest')\n",
    "    plt.title(\"Confusion Matrix - Random Forest\")\n",
    "    plt.colorbar()\n",
    "    plt.xticks(ticks=np.arange(len(labels_sorted)),\n",
    "               labels=labels_sorted, rotation=90)\n",
    "    plt.yticks(ticks=np.arange(len(labels_sorted)), labels=labels_sorted)\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(REPORT_DIR, \"rf_confusion_matrix.png\"))\n",
    "    plt.show()\n",
    "\n",
    "    # Per-genre performance metrics\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_test, y_pred, labels=labels_sorted, zero_division=0)\n",
    "\n",
    "    metrics = {\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Support': support\n",
    "    }\n",
    "\n",
    "    for metric_name, values in metrics.items():\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.bar(labels_sorted, values)\n",
    "        plt.ylabel(metric_name)\n",
    "        plt.title(f\"Per-Genre {metric_name} - RF\")\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        filename = f\"rf_per_genre_{metric_name.lower().replace('-', '_')}.png\"\n",
    "        plt.savefig(os.path.join(REPORT_DIR, filename))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
